# 5 â€” Testing Strategy per Progetti Python

> Prompt universale per analizzare la copertura test, identificare gap critici,
> e costruire una strategia di testing completa su qualsiasi progetto Python.
> Compatibile con qualsiasi LLM che abbia accesso al filesystem e al terminale.

---

## Prompt

```
Ruolo: Agisci come un QA Engineer / Test Architect specializzato in Python.

Contesto: Devo analizzare lo stato attuale dei test di un progetto Python,
identificare i gap di copertura piÃ¹ critici, e produrre una strategia
per raggiungere una copertura robusta e mantenibile.

Obiettivo: Analizza test esistenti, misura coverage, identifica moduli
non testati, e produci un piano d'azione con prioritÃ .

IMPORTANTE: Questo prompt Ã¨ GENERICO. Adattati al progetto che trovi.

=============================================================================
FASE 0 â€” RICOGNIZIONE TEST ESISTENTI
=============================================================================

1. IDENTIFICA il framework di test:
   ```bash
   # Cerca configurazione pytest
   grep -r "pytest" pyproject.toml setup.cfg tox.ini 2>/dev/null

   # Cerca configurazione unittest
   grep -r "unittest" src/ --include="*.py" -l 2>/dev/null
   ```

2. MAPPA la struttura dei test:
   ```bash
   # Tutti i file di test
   find tests/ -name "test_*.py" -o -name "*_test.py" 2>/dev/null

   # Conta test per file
   grep -c "def test_" tests/**/*.py 2>/dev/null | sort -t: -k2 -rn
   ```

3. IDENTIFICA i file sorgente:
   ```bash
   # Tutti i moduli Python sorgente (esclusi test)
   find src/ -name "*.py" -not -name "test_*" -not -name "__pycache__" 2>/dev/null
   ```

4. CALCOLA il rapporto di copertura strutturale:
   Per ogni file in src/, verifica se esiste un corrispondente test_*.py:

   ```
   src/modulo.py          â†’ tests/test_modulo.py         âœ… coperto
   src/altro.py           â†’ tests/test_altro.py          âŒ MANCANTE
   src/sub/helper.py      â†’ tests/sub/test_helper.py     âŒ MANCANTE
   ```

   File senza test corrispondente â†’ GAP da colmare.

=============================================================================
FASE 1 â€” COVERAGE QUANTITATIVA
=============================================================================

1. ESEGUI pytest con coverage:
   ```bash
   pytest --cov=src --cov-report=term-missing --cov-report=html -v
   ```

   Se pytest non Ã¨ configurato:
   ```bash
   pip install pytest pytest-cov
   pytest tests/ --cov=src --cov-report=term-missing -v
   ```

2. ANALIZZA il report coverage:

   Soglie di riferimento:
   - < 30%:  Copertura critica â€” rischio alto di regressioni
   - 30-50%: Copertura bassa â€” funzionalitÃ  core probabilmente scoperte
   - 50-70%: Copertura accettabile â€” focus su branch coverage
   - 70-85%: Copertura buona â€” target per la maggior parte dei progetti
   - > 85%:  Copertura eccellente â€” mantenere, non inseguire il 100%

3. IDENTIFICA i moduli con coverage piÃ¹ bassa:
   - Elenca i 10 file con coverage peggiore
   - Per ognuno, nota le righe "Missing" dal report
   - Classifica per criticitÃ  (vedi Fase 3)

4. ANALIZZA branch coverage (non solo line coverage):
   ```bash
   pytest --cov=src --cov-branch --cov-report=term-missing
   ```
   Branch coverage Ã¨ piÃ¹ significativa di line coverage perchÃ©
   verifica che ogni ramo di ogni if/else sia stato esercitato.

=============================================================================
FASE 2 â€” ANALISI QUALITATIVA DEI TEST
=============================================================================

Non basta avere tanti test. I test devono essere BUONI.

1. VERIFICA test quality per ogni file di test:

   a. NOMI DESCRITTIVI:
      ```
      âœ… def test_calculate_bmi_returns_correct_value_for_normal_weight():
      âŒ def test_1():
      âŒ def test_calc():
      ```

   b. ARRANGE-ACT-ASSERT (AAA):
      Ogni test dovrebbe avere tre sezioni chiare:
      - Arrange: setup dei dati
      - Act: esecuzione dell'azione
      - Assert: verifica del risultato

      Cerca test senza assert:
      ```bash
      # Test che non hanno assert (possibili test vuoti/incompleti)
      grep -L "assert" tests/**/*.py 2>/dev/null
      ```

   c. UN ASSERT PER TEST (ideale):
      ```bash
      # Test con troppi assert (> 5) â€” possibili God Tests
      for f in tests/**/*.py; do
        awk '/def test_/{name=$0; count=0} /assert/{count++} /^def |^class /{if(count>5) print name, count}' "$f" 2>/dev/null
      done
      ```

   d. INDIPENDENZA:
      - I test dipendono dall'ordine di esecuzione? â†’ FRAGILE
      - I test condividono stato mutabile? â†’ FRAGILE
      - I test dipendono da risorse esterne (DB, API, file)? â†’ SLOW + FRAGILE

2. CERCA ANTI-PATTERN nei test:

   a. TEST CHE NON TESTANO NULLA:
      ```bash
      # Funzioni test senza corpo
      grep -A2 "def test_" tests/**/*.py 2>/dev/null | grep -B1 "pass$"
      ```

   b. MOCK ECCESSIVO:
      ```bash
      # File con troppi mock (> 10 per file)
      grep -c "mock\|patch\|MagicMock" tests/**/*.py 2>/dev/null | \
        awk -F: '$2 > 10 {print}'
      ```
      Troppi mock â†’ i test non testano il comportamento reale.

   c. TEST FRAGILI (dipendono da valori esatti):
      ```bash
      # Assert su stringhe esatte (fragili se cambiano messaggi)
      grep -rnE 'assert.*==.*["\'].*["\']' tests/ --include="*.py" | head -10
      ```

   d. FIXTURE MANCANTI:
      - Setup ripetuto in ogni test â†’ estrarre in fixture/conftest.py
      ```bash
      grep -c "conftest.py" tests/**/* 2>/dev/null
      # Se 0, probabilmente mancano fixture condivise
      ```

=============================================================================
FASE 3 â€” PRIORITIZZAZIONE DEI GAP
=============================================================================

Non tutti i gap di coverage hanno la stessa importanza.
Classifica per RISCHIO BUSINESS, non per percentuale.

PRIORITÃ€ ALTA (testare PRIMA):
- Logica di business core (calcoli, trasformazioni dati)
- Gestione dati utente (salvataggio, caricamento, migrazione)
- Integrazioni esterne (API, database, file I/O)
- Parsing/validazione input
- Gestione errori e edge case
- Flussi che toccano dati finanziari o sensibili

PRIORITÃ€ MEDIA:
- Utility functions e helper
- Conversioni formato/unitÃ 
- Logica di presentazione (formatting)
- Configurazione e setup

PRIORITÃ€ BASSA:
- Codice GUI puramente visuale (layout, colori, font)
- Boilerplate (__repr__, __str__)
- Codice generato automaticamente
- Script one-off

PER OGNI GAP, documenta:
```
File: src/modulo.py
Coverage attuale: 45%
Righe mancanti: 23-45, 78-92, 110-130
PrioritÃ : ALTA
Motivo: Contiene logica di calcolo core usata ovunque
Test necessari:
  - test caso base con input valido
  - test edge case (input vuoto, None, valori estremi)
  - test errore (input invalido, eccezioni attese)
Effort stimato: 2h
```

=============================================================================
FASE 4 â€” STRATEGIA DI FIXTURE E CONFTEST
=============================================================================

Un buon sistema di fixture riduce la duplicazione nei test e li rende
piÃ¹ leggibili e mantenibili.

1. ANALIZZA conftest.py esistente (se esiste):
   ```bash
   find tests/ -name "conftest.py" 2>/dev/null
   ```
   - Quante fixture sono definite?
   - Sono usate effettivamente nei test?
   - Coprono i casi d'uso principali (DB, mock API, dati di test)?

2. FIXTURE RACCOMANDATE per tipo di progetto:

   Database (SQLAlchemy/Django ORM):
   ```python
   @pytest.fixture
   def db_session():
       """Database session con rollback automatico."""
       # Setup: crea sessione in-memory o transaction
       # Yield: passa la sessione al test
       # Teardown: rollback automatico

   @pytest.fixture
   def sample_data(db_session):
       """Dati di esempio pre-popolati."""
       # Inserisci record di test
   ```

   API esterne:
   ```python
   @pytest.fixture
   def mock_api(requests_mock):  # o responses
       """Mock delle API esterne."""
       # Registra risposte mock per ogni endpoint usato

   @pytest.fixture
   def api_response_success():
       """Payload di risposta API di successo."""
       return {"status": "ok", "data": [...]}
   ```

   File system:
   ```python
   @pytest.fixture
   def tmp_config(tmp_path):
       """File di configurazione temporaneo."""
       config = tmp_path / "config.json"
       config.write_text('{"key": "value"}')
       return config
   ```

3. ORGANIZZAZIONE conftest.py:
   ```
   tests/
   â”œâ”€â”€ conftest.py           # Fixture globali (db, mock generici)
   â”œâ”€â”€ data/
   â”‚   â””â”€â”€ conftest.py       # Fixture per test data layer
   â”œâ”€â”€ logic/
   â”‚   â””â”€â”€ conftest.py       # Fixture per test logica
   â””â”€â”€ integration/
       â””â”€â”€ conftest.py       # Fixture per test integrazione
   ```

=============================================================================
FASE 5 â€” PATTERN DI TEST RACCOMANDATI
=============================================================================

Per ogni tipo di codice, suggerisci il pattern di test appropriato:

1. FUNZIONI PURE (input â†’ output, no side effect):
   ```python
   @pytest.mark.parametrize("input_val, expected", [
       (caso_base, risultato_atteso),
       (edge_case_1, risultato_edge_1),
       (edge_case_2, risultato_edge_2),
   ])
   def test_funzione(input_val, expected):
       assert funzione(input_val) == expected
   ```
   Usa @parametrize per coprire piÃ¹ casi senza duplicare codice.

2. CLASSI CON STATO:
   ```python
   class TestMiaClasse:
       def test_stato_iniziale(self):
           obj = MiaClasse()
           assert obj.valore == default

       def test_modifica_stato(self):
           obj = MiaClasse()
           obj.aggiorna(nuovo_valore)
           assert obj.valore == nuovo_valore

       def test_invariante_mantenuto(self):
           obj = MiaClasse()
           obj.operazione_complessa()
           assert obj.invariante_valido()
   ```

3. CODICE CON SIDE EFFECT (DB, file, API):
   ```python
   def test_salva_record(db_session):
       # Arrange
       record = Record(nome="test")

       # Act
       salva(db_session, record)

       # Assert
       risultato = db_session.query(Record).first()
       assert risultato.nome == "test"
   ```

4. ECCEZIONI:
   ```python
   def test_errore_input_invalido():
       with pytest.raises(ValueError, match="deve essere positivo"):
           funzione(valore_negativo=-5)
   ```

5. INTEGRAZIONE API ESTERNE:
   ```python
   def test_fetch_dati(mock_api):
       mock_api.get("https://api.example.com/data", json={"ok": True})
       risultato = client.fetch_dati()
       assert risultato["ok"] is True
   ```

=============================================================================
FASE 6 â€” PIANO DI TESTING
=============================================================================

Produci un piano ordinato per prioritÃ . Formato:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               TESTING STRATEGY REPORT                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Progetto:             <nome>                                â•‘
â•‘ Data:                 <data>                                â•‘
â•‘ Coverage attuale:     N%  (line) / N% (branch)              â•‘
â•‘ Test totali:          N                                     â•‘
â•‘ File testati:         N / M  (N%)                           â•‘
â•‘ Tempo esecuzione:     Ns                                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘ COVERAGE PER MODULO:                                        â•‘
â•‘  src/core.py           95%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  âœ…        â•‘
â•‘  src/logic.py          72%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  âš ï¸        â•‘
â•‘  src/database.py       45%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  âŒ        â•‘
â•‘  src/api.py            12%  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ğŸ”´        â•‘
â•‘  src/gui.py             0%  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â¬›        â•‘
â•‘                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘ GAP CRITICI (PrioritÃ  ALTA):                                â•‘
â•‘                                                              â•‘
â•‘  1. src/database.py (45% â†’ target 80%)                      â•‘
â•‘     Mancano: test upsert, test migration, test rollback     â•‘
â•‘     Test da scrivere: ~12                                    â•‘
â•‘     Fixture necessarie: db_session, sample_records           â•‘
â•‘                                                              â•‘
â•‘  2. src/api.py (12% â†’ target 70%)                           â•‘
â•‘     Mancano: test error handling, test timeout, test retry   â•‘
â•‘     Test da scrivere: ~8                                     â•‘
â•‘     Fixture necessarie: mock_api, api_responses              â•‘
â•‘                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘ QUALITÃ€ TEST ESISTENTI:                                     â•‘
â•‘  Naming:              âœ… / âš ï¸ / âŒ                           â•‘
â•‘  AAA pattern:         âœ… / âš ï¸ / âŒ                           â•‘
â•‘  Indipendenza:        âœ… / âš ï¸ / âŒ                           â•‘
â•‘  Fixture usage:       âœ… / âš ï¸ / âŒ                           â•‘
â•‘  Parametrize usage:   âœ… / âš ï¸ / âŒ                           â•‘
â•‘  Anti-pattern trovati: N                                    â•‘
â•‘                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘ PIANO DI AZIONE:                                             â•‘
â•‘                                                              â•‘
â•‘  Sprint 1 â€” Foundation (setup + quick wins):                â•‘
â•‘   â–¡ Creare/aggiornare conftest.py con fixture base          â•‘
â•‘   â–¡ Fix test fragili/broken esistenti                       â•‘
â•‘   â–¡ Aggiungere test per funzioni pure (piÃ¹ facili)          â•‘
â•‘   Target: +N% coverage                                      â•‘
â•‘                                                              â•‘
â•‘  Sprint 2 â€” Core business logic:                             â•‘
â•‘   â–¡ Test modulo X (logica core)                             â•‘
â•‘   â–¡ Test modulo Y (calcoli/trasformazioni)                  â•‘
â•‘   â–¡ Edge case e error handling                              â•‘
â•‘   Target: +N% coverage                                      â•‘
â•‘                                                              â•‘
â•‘  Sprint 3 â€” Integration:                                     â•‘
â•‘   â–¡ Test database operations (con mock/in-memory)           â•‘
â•‘   â–¡ Test API esterne (con mock)                             â•‘
â•‘   â–¡ Test file I/O                                           â•‘
â•‘   Target: +N% coverage                                      â•‘
â•‘                                                              â•‘
â•‘  Coverage target finale: N%                                  â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

=============================================================================
PRINCIPI GUIDA
=============================================================================

1. TESTA COMPORTAMENTO, NON IMPLEMENTAZIONE: I test devono verificare
   COSA fa il codice, non COME lo fa. Se cambi l'implementazione
   mantenendo lo stesso risultato, i test non dovrebbero rompersi.

2. PIRAMIDE DEI TEST: Tanti unit test (veloci, isolati),
   alcuni integration test (piÃ¹ lenti, piÃ¹ realistici),
   pochi end-to-end test (lenti, fragili ma completi).

3. COVERAGE â‰  QUALITÃ€: 100% coverage con assert banali Ã¨ peggio
   di 70% coverage con test significativi. Prioritizza la qualitÃ .

4. TEST COME DOCUMENTAZIONE: Un buon test spiega come usare il codice.
   Leggendo il test, dovrebbe essere chiaro il contratto della funzione.

5. FAIL FAST: I test devono fallire velocemente e con messaggi chiari.
   Usa messaggi custom negli assert per debugging piÃ¹ rapido.

6. ISOLA LE DIPENDENZE: Mocka le dipendenze esterne (DB, API, file system)
   nei unit test. Usa le dipendenze reali solo negli integration test.
```
